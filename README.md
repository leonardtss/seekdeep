# D√©ploiement de l'application  

Ce d√©p√¥t contient le code associ√© √† la vid√©o üëâ [YouTube](https://youtu.be/y3K4hji9W8g).  

## Pr√©requis  

L'application repose sur **quatre blocs**, deux sont optionnels, ils permettent de stocker les conversations. Voici la liste des blocs :  

‚úÖ **Frontend** : Application en **React.js**  
‚úÖ **LLM** : Mod√®le d√©ploy√© en local ou sur une instance lou√©e chez un fournisseur cloud  
üü° **API** (optionnel) : Backend en **Express.js**  
üü° **Base de donn√©es** (optionnel) : **MongoDB** pour stocker les conversations  

## D√©ploiement  


### 1Ô∏è‚É£ D√©ploiement du LLM  

Je recommande de suivre la vid√©o pour plus de d√©tails, voici les grandes lignes :  

Si vous louez une instance GPU chez **Scaleway, OVH, AWS ou GCP**, connectez-vous en **SSH**, puis ex√©cutez la commande suivante pour lancer un mod√®le avec **vLLM** :  

```bash
docker run --runtime nvidia --gpus all     -v ~/.cache/huggingface:/root/.cache/huggingface     -p 8000:8000     --ipc=host     vllm/vllm-openai:latest     --model modelName1     --max-model-len 10000
```

**Param√®tres cl√©s** :  
- **`--model modelName1`** : Remplacez par le mod√®le souhait√© en fonction de vos besoins et de la puissance de votre machine, vous trouverez un tr√®s grand nombre de mod√®les sur [Hugging Face](https://huggingface.co/models).  
- **`--max-model-len 10000`** : Ajustez en fonction des capacit√©s de votre instance.  

### Ajouter un second mod√®le  

Si vous souhaitez un **mod√®le de r√©flexion** comme ce qui est fait chez DeepSeek, vous pouvez lancer un **second mod√®le** :  

#### Sur la **m√™me instance** (si la capacit√© le permet)  

```bash
docker run --runtime nvidia --gpus all     -v ~/.cache/huggingface:/root/.cache/huggingface     -p 8001:8000     --ipc=host     vllm/vllm-openai:latest     --model modelName2     --max-model-len 10000
```

#### Sur **une autre instance** (si la premi√®re est trop limit√©e)  
```bash
docker run --runtime nvidia --gpus all     -v ~/.cache/huggingface:/root/.cache/huggingface     -p 8000:8000     --ipc=host     vllm/vllm-openai:latest     --model modelName2     --max-model-len 10000
```

**Param√®tres cl√©s** :  
- **`--model modelName2`** : Remplacez par le mod√®le souhait√© en fonction de vos besoins et de la puissance de votre machine, vous trouverez un tr√®s grand nombre de mod√®les sur [Hugging Face](https://huggingface.co/models).  
- **`--max-model-len 10000`** : Ajustez en fonction des capacit√©s de votre instance.  

### 2Ô∏è‚É£D√©ploiement du Frontend  

Rendez-vous dans le fichier DeepseekInput.jsx qui se trouve dans front/src/. Dans ce fichier vous devez renseigner les variables : **modelName1**, **modelName2**, **baseModelUrl1**, **baseModelUrl2**. baseModelUrl1 et baseModelUrl2 sont de la forme adresse "ip de l'instance":"port expos√©" (par exemple http://51.159.135.40:8000).

Il vous reste plus qu'√† lancer l'application avec les commandes suivantes:

```bash
cd front
npm install
npm run dev
```

Par d√©faut, l'application tourne sur **localhost:5173**.


### 3Ô∏è‚É£ (Optionnel) Stockage des conversations avec MongoDB  

Si vous souhaitez **m√©moriser les conversations**, vous avez besoin de :  
- Une **base de donn√©es MongoDB** (MongoDB Atlas recommand√©)  
- Une **API** pour faire le lien entre le front et la base de donn√©es  

#### Configuration de MongoDB Atlas  

1. Cr√©ez un **cluster**, un **utilisateur admin** et un **mot de passe**.  
2. R√©cup√©rez l'**URL de connexion** et placez-la dans un fichier **.env** que vous devez cr√©er dans le dossier  `api` et nommez cette variable MONGODB_URL.  
3. Cr√©ez un utilisateur dans la collection **users**. Vous avez seulement besoin de renseigner le **username**, mongodb va ensuite cr√©er un id. Cet id il faut le r√©cup√©rer et le stocker dans la variable **userId** qui se trouve dans front/App.jsx.

#### D√©ploiement de l‚ÄôAPI  
Pour d√©ployer l'api ex√©cutez les commandes suivantes.

```bash
cd api
npm install
npm run dev
```

## Acc√®s √† l‚Äôapplication  

Une fois tout d√©ploy√©, rendez-vous sur **localhost:5173** et commencez √† interagir avec vos LLMs ! üöÄ
